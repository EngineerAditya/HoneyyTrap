"""
Honeypot API - Main FastAPI Application
"""

import os
import json
import requests
from fastapi import FastAPI, Header, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv

import google.generativeai as genai

from .models import HoneypotRequest, HoneypotResponse, ErrorResponse
from .intelligence import IntelligenceExtractor, session_store
from .agent.manager import AgentManager
from .agent.states import AgentState


# --------------------------------------------------
# ENV + CONFIG
# --------------------------------------------------
load_dotenv()

API_KEY = os.getenv("HONEYPOT_API_KEY", "test-api-key-change-me")
GUVI_ENDPOINT = "https://hackathon.guvi.in/api/updateHoneyPotFinalResult"


# Agent Manager handles Gemini config now



# --------------------------------------------------
# FASTAPI APP
# --------------------------------------------------
app = FastAPI(
    title="Honeypot Scam Detection API",
    description="AI-powered honeypot for detecting scams and extracting intelligence",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

extractor = IntelligenceExtractor()
agent = AgentManager()


# --------------------------------------------------
# AUTH
# --------------------------------------------------
def verify_api_key(x_api_key: str = Header(...)):
    if x_api_key != API_KEY:
        raise HTTPException(status_code=401, detail="Invalid API key")
    return x_api_key


# --------------------------------------------------
# HEALTH
# --------------------------------------------------
@app.get("/health")
def health_check():
    return {"status": "healthy", "message": "Honeypot API is running"}


# --------------------------------------------------
# GEMINI REPLY (LLM ONLY TALKS)
# --------------------------------------------------

# Local LLM function removed in favor of AgentManager



# --------------------------------------------------
# MAIN ENDPOINT
# --------------------------------------------------
@app.post(
    "/honeypot",
    response_model=HoneypotResponse,
    responses={
        401: {"model": ErrorResponse},
        422: {"model": ErrorResponse}
    }
)
def process_message(
    request: HoneypotRequest,
    api_key: str = Depends(verify_api_key)
):
    session_id = request.sessionId

    print(f"\n[SESSION: {session_id}] Message: {request.message.text}")

    # --------------------------------------------------
    # STATELESS SESSION BACKFILL (ROBUSTNESS)
    # --------------------------------------------------
    # Always try to backfill from history to handle restarts/statelessness
    session = session_store.backfill_history(
        session_id=session_id, 
        history=request.conversationHistory,
        extractor=extractor
    )
    
    # Reset NOT needed anymore as backfill handles it
    # if len(request.conversationHistory) == 0:
    #     session_store.clear_session(session_id)

    # --------------------------------------------------
    # INTELLIGENCE EXTRACTION
    # --------------------------------------------------
    current_intel = extractor.extract(request.message.text)
    session = session_store.add_intelligence(session_id, current_intel)

    print(f"[SESSION: {session_id}] Scam detected: {session.scam_detected}")
    print(f"[SESSION: {session_id}] Message count: {session.message_count}")

    # --------------------------------------------------
    # TERMINATION LOGIC (CRITICAL)
    # --------------------------------------------------
    should_terminate = False

    # Non-scam: stop early
    if not session.scam_detected and session.message_count >= 5:
        should_terminate = True

    # Scam honeypot: extended conversation
    elif session.scam_detected and session.message_count >= 15:
        should_terminate = True

    # --------------------------------------------------
    # TERMINATE â†’ GUVI CALLBACK
    # --------------------------------------------------
    if should_terminate:
        payload = session_store.get_final_payload(
            session_id,
            agent_notes="Auto-generated by Agentic Honeypot"
        )

        print("\n[CALLBACK PAYLOAD]")
        print(json.dumps(payload, indent=2))

        try:
            requests.post(GUVI_ENDPOINT, json=payload, timeout=5)
        except Exception as e:
            print(f"[CALLBACK ERROR] {e}")

        return HoneypotResponse(
            status="success",
            reply="Thank you. I will check this and get back later."
        )

    # --------------------------------------------------
    # LLM CONTEXT + REPLY
    # --------------------------------------------------
    # llm_context generation is now handled inside AgentManager
    
    try:
        ai_reply = agent.generate_response(
            session_id=session_id,
            user_text=request.message.text
        )
    except Exception as e:
        print(f"[AGENT ERROR] {e}")
        ai_reply = "I am having some trouble with my network. Can you repeat?"

    return HoneypotResponse(
        status="success",
        reply=ai_reply
    )


# --------------------------------------------------
# RUN
# --------------------------------------------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)